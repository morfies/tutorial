#### js中的iterator

有next()方法，返回值有value和done两个属性，当done:true时，遍历结束

自制一个迭代器:
```
function makeIterator(arr) {
  let nextIndex = 0;
  return {
    next: () => {
      return nextIndex < arr.length ? { done: false, value: arr[nextIndex++] } : { done: true }
    }
  };
}
```

- generator:
```
function* idMaker() {
  let index = 0;
  while(1) {
    yield ++index;
  }
}
```
- 可迭代Iterable:
```
var myIterable = {};
myIterable[Symbol.iterator] = function* () {
    yield 1;
    yield 2;
    yield 3;
};

for (let value of myIterable) { 
    console.log(value); 
}
// 1
// 2
// 3

// or

[...myIterable]; // [1, 2, 3]
```


#### js模块化规范CommonJS, AMD, CMD
CommonJS的require是同步加载的，npm即实现的是CommonJS，比如

```
    const math = require('math')
    math.add(2, 3)
```
如果在浏览器环境，文件是从服务器加载的，这种同步模式会导致阻塞，于是就有了异步加载规范，即AMD:
> require(module, callback)

requirejs实现的是AMD，与seajs的异同：
requirejs 是先加载所有require文件，完了再执行逻辑；
seajs则是执行到具体的require文件时就加载，即思维更像同步模式


#### nodejs运用场景:
- 前后端分离
> 前端：负责View和Controller层
> 后端：只负责model层，业务处理/数据等
试想一下，如果前端掌握了Controller，我们可以做url design，我们可以根据场景决定在服务端同步渲染，还是根据view层数据输出json数据，我们还可以根据表现层需求很容易的做bigpipe,comet,socket等等，完全是需求决定使用方式。

如果没有node这一层，则主要会是在java后端进行MVC模式开发，这种模式严重阻碍了前端开发效率，也使得后端不能专注业务开发。

- 一个页面有很多块数据是实时通过ajax获取，如果后端是java，那么可能需要起多个http请求取数，如果前端能直接在node层做代理封装，前端只用一次http，node层做数据整合，性能提升会很多，因为无线端建立一个http请求是消耗很大的
- 



#### Spring中的依赖注入 
并不是一种技术，而是一种设计思想
通过简单的语法，能够实现Ioc容器来控制对象的生命周期，且对象要依赖其它对象时，只需简单申明，这种设计思路，就使得在编写组件时，需要独立和解耦，最终达到的效果是，应用更松耦合、灵活以及可拓展性更强

#### 压测及系统性能分析
预估流量(DAU-PV-时间分布-QPS(**二八原则,线上80%的流量发生在一天20%的时间内**))->接口梳理及接口的依赖分析->接口流量分布，制定压测指标
压测期间，不能影响正常用户体验，所以对系统指标有要求，比如:CPU<=70%;响应时间<=100ms;load1值<CPU 核数-0.5;内存<=80%
> load1解释：1分钟内的cpu平均负载，比如load1=5则意味着一共有5个process在使用或者等待cpu资源  

- 压测工具：ab,wrk2
- 代码片段的性能测试模块: `benchmark` + `beautify-benchmark`
> (比如可以对比测试正则的test和String的indexOf性能，还可以根据这模块测出es6的class extends继承方式没有util.extends和prototype实现的继承性能好，底层模块可以考虑使用util.extends，业务代码则可以使用extends方式，优雅，且性能牺牲不算多)

> 压测过程中观察指标：系统指标(cpu、内存(泄露)、响应时间、数据库慢查询) + cpu profile(比如alinode的profile功能，可以在QPS高的阶段dump，然后就可以很好的分析出代码的性能瓶颈)

> 性能优化：缓存空间换时间、热点代码的算法优化(时间复杂度)、业务逻辑优化避免热点代码等


#### 两阶段提交
https://blog.csdn.net/sofia1217/article/details/53968177
即分布式事务中，先投票，再执行提交的过程，缺点是协调节点是单点，且过程中有阻塞等待，效率低；然后还是可能发生数据不一致的情况。



#### node的场景
rpc相关:

#### egg进程模型
直接使用cluster方式简单，各worker可以同时监听同一端口，master进程只负责其它进程的启动和异常恢复和监控等，worker进程都运行同一份代码，负责处理具体的请求。
问题：
>worker进程异常退出后该如何处理？
>worker间如何共享资源？
>多worker之间如何调度？通信？

基于cluster的模型，其实可以满足绝大部分场景，但是对于一些公共事务，如日志管理等，如果所有worker都来做，没必要，于是，egg后来加入了agentWorker，专门来做这些公共事务。整体egg应用的启动时序是这样的:
```
+---------+           +---------+          +---------+
|  Master |           |  Agent  |          |  Worker |
+---------+           +----+----+          +----+----+
     |      fork agent     |                    |
     +-------------------->|                    |
     |      agent ready    |                    |
     |<--------------------+                    |
     |                     |     fork worker    |
     +----------------------------------------->|
     |     worker ready    |                    |
     |<-----------------------------------------+
     |      Egg ready      |                    |
     +-------------------->|                    |
     |      Egg ready      |                    |
     +----------------------------------------->|
```

IPC通讯模型：
```
广播消息： agent => all workers
                  +--------+          +-------+
                  | Master |<---------| Agent |
                  +--------+          +-------+
                 /    |     \
                /     |      \
               /      |       \
              /       |        \
             v        v         v
  +----------+   +----------+   +----------+
  | Worker 1 |   | Worker 2 |   | Worker 3 |
  +----------+   +----------+   +----------+

指定接收方： one worker => another worker
                  +--------+          +-------+
                  | Master |----------| Agent |
                  +--------+          +-------+
                 ^    |
     send to    /     |
    worker 2   /      |
              /       |
             /        v
  +----------+   +----------+   +----------+
  | Worker 1 |   | Worker 2 |   | Worker 3 |
  +----------+   +----------+   +----------+
```

基于上面的通讯模型，egg在process对象上封装了messenger对象，抽象了IPC的复杂性，通过sendToApp/sendToAgent/broadcast/sendTo/sendRandom等方法轻松搞定进程间通讯。
有了这套通讯机制，就能实现一些场景，比如通过agent保持与远程服务器的心跳和更新缓存，并通知worker刷新本地缓存等。

#### 进程守护
鲁棒性，nodejs中进程退出分为两类：
- 未捕获的异常，通过`process.on('uncaughtException', handler)`来捕获它，但当一个worker遇到未捕获异常时，已处于不确定状态，需要优雅退出
    + 关闭异常进程所有的TCP server(将已有连接快速断开，且不再接收新连接)，断开和Master的IPC通道，不再接收新的用户请求
    + Master立即fork一个新的worker，维持总worker数不变
    + 异常worker等待一段时间，处理完已经接收的请求后退出(请求触发的一些计算任务等)
- OOM及系统异常，当遇到这类异常时，进程会直接被系统杀死，我们只能直接重新fork新的worker
**egg中使用graceful和egg-cluster两个模块来完成上述目标**
graceful代码解读:
1. 监听会导致进程挂掉的uncaughtException，在callback中做下面的事情
2. 监听所有server的`request`事件，并关闭连接
```
req.shouldKeepAlive = false;
res.shouldKeepAlive = false;
res.setHeader('Connection', 'close');
```
3. 设置超时机制，在超时后认为当前进程已经处理完残留请求，并直接process.exit(1);
4. 通过`cluster.worker`判断是否是cluster模式下的worker进程，若是
5. 所有servers需要`server.close()`
6. 当前worker需要发送IPC消息到master `worker.send('graceful:disconnect')`
7. 当前`worker.disconnect()`. 按照API文档，worker在disconnect时，会将当前进程的所有server做close处理的，上面第5步看起来多余。

#### 源码解读
##### cfork:
会判断在master进程中fork出需要的worker进程，并在里面监听了disconnect和exit事件，其中有逻辑判断两者发生的先后顺序(worker.isDead()为true，则表明先执行到了exit事件)，在这两个事件中都有refork的逻辑，为了防止重复refork，所以需要判断去重
1. 第一步判断是否是master进程，只允许在master中fork出worker进程
2. 接着是参数处理，主要用于setupMaster(settings)方法，即指定fork出的worker的具体配置
3. 接下来是绑定两大cluster中监听worker退出的事件，disconnect 和 exit
  - 实验表明，disconnect不一定触发，但exit在worker死掉时一定会触发，那么这个库考虑了两者，主要是记录了分别发生的次数，并在两个监听handler中都refork了worker进程，个人理解只在exit中refork应该也可以实现逻辑；同时还有防止重复fork的逻辑，即通过worker.process.pid放在数组中，判断该worker是否已经经历过refork的过程。
4. 最后是defer了cluster中自己emit的一些事件的监听，这个不知为何这么写?

##### sendmessage:
这个库逻辑挺简单的，核心基础就是cluster模式下，master和worker之间(父子进程间)是有IPC连接的(通过isConnected()判断是否连接正常)，只要连接正常，都可以用send的方式发送IPC消息，以`.on('message', fn)`的方式接受消息。
此外，需要理解在cluster模式下，调用send()方法的主体是有相对概念的，即，在master中，需要通过`cluster.worker.process.send()`就能发给worker，在worker中，通过`process.send()`就能发送给master。
这个库里面把这种区别屏蔽掉了，用起来相对方便一点



#### 消息监听`.once`和`.on`的区别
通过.once()绑定的listener，在触发一次后，会被detach掉.`The next time eventName is triggered, this listener is removed and then invoked.`

#### cluster模块
node进程模型本来是单核单进程的，就会导致任务只能使用一个cpu的能力，在node7之后，cluster模块的出现，使得node可以轻松使用多核CPU的所有核心资源，且cluster模式下，master和worker之间可以通过IPC通信(在fork子进程时，通讯需要设置为ipc，fork后，父子进程除了IPC信道外，其它包括存储空间、进程上下文等等都是独立的，并不是Linux系统中的复制进程的方式)


#### 部署脚本
- 启动
检测依赖资源、通知LB
- 退出
释放依赖资源、通知LB


#### nodejs几种部署性能对比:
笔者做了压测(ApacheBench)，分别看看几种模式的性能
- nginx + node cluster 配置简单(proxy_pass 需要为长连接模式)
- nginx + node worker 配置复杂，需要额外工具
结论：第二种性能高，但配置复杂；第一种性能几乎减半。。。

http://jaminzhang.github.io/linux/understand-Linux-backlog-and-somaxconn-kernel-arguments/
文章介绍了linux `backlog/somaxconn`参数：
`TCP`建立连接需要经过三次握手，在客户端向服务端建立连接的过程中，服务端通常会经历两个tcp状态：`SYN_REVD`,`ESTABLISHED`.
对应的也会维护两个队列：**半连接队列(存放SYN的队列)和全连接队列(Established队列)**
后端应用程序则只从全连接队列中获取连接并处理。

`backlog/somaxconn`参数则是调整这两个队列大小的：
**全连接队列长度** = min(`backlog`, 内核参数 `net.core.somaxconn`)，`net.core.somaxconn` 默认为 128。
这个很好理解，`net.core.somaxconn` 定义了系统级别的全连接队列最大长度，
`backlog` 只是应用层传入的参数，不可能超过内核参数，所以 `backlog` 必须小于等于 `net.core.somaxconn`
**半连接队列长度** = min(`backlog`, 内核参数 `net.core.somaxconn`，内核参数 `tcp_max_syn_backlog`)。



#### 最经典的TCP性能问题:
**delay ack**: 指收到包后不立即ack，而是等一小会（比如40毫秒）看看，如果这40毫秒以内正好有一个包（比如上面的http response）发给client，那么我这个ack包就跟着发过去（顺风车，http reponse包不需要增加任何大小），这样节省了资源。 当然如果超过这个时间还没有包发给client（比如nginx处理需要40毫秒以上），那么这个ack也要发给client了（即使为空，要不client以为丢包了，又要重发http request，划不来）。
假如这个时候ack包还在等待延迟发送的时候，又收到了client的一个包，那么这个时候server有两个ack包要回复，那么os会把这两个ack包合起来立即回复一个ack包给client，告诉client前两个包都收到了。
也就是delay ack开启的情况下：ack不立即发而是等40毫秒，等的过程中ack包有顺风车就搭；或者如果凑够两个ack包自己包个车也立即发车；再如果等了40毫秒以上也没顺风车，那么自己打个车也发车。

delay ack在nginx中是有参数可以关闭的，一般推荐关闭.

- Nagle算法:
```
if there is new data to send
  if the window size >= MSS and available data is >= MSS
        send complete MSS segment now
  else
    if there is unconfirmed data still in the pipe
        enqueue data in the buffer until an acknowledge is received
    else
        send data immediately
    end if
  end if
end if
```
这段代码的意思是如果接收窗口大于MSS 并且 要发送的数据大于 MSS的话，立即发送。
否则：
看看前面发出去的包是不是还有没有ack的，如果有没有ack的那么我这个小包不急着发送，等前面的ack回来再发送
我总结下Nagle算法逻辑就是：如果发送的包很小（不足MSS），又有包发给了对方，对方还没回复说收到了，那我也不急着发，等前面的包回复收到了再发。这样可以优化带宽利用率（早些年带宽资源还是很宝贵的），Nagle算法也是用来优化改进tcp传输效率的。

delay ack 和 Nagle两者同时起作用时，就可能形成相互等待，导致网络rt变高。
经典案例 http://www.stuartcheshire.org/papers/nagledelayedack/
案例核心奇怪的问题是，如果传输的数据是 99,900 bytes，速度5.2M/秒； 
如果传输的数据是 100,000 bytes 速度2.7M/秒，多了10个bytes，不至于传输速度差这么多。

原因就是：
>99,900 bytes = 68 full-sized 1448-byte packets, plus 1436 bytes extra
>100,000 bytes = 69 full-sized 1448-byte packets, plus   88 bytes extra

99,900 bytes：
68个整包会立即发送，因为68是偶数，对方收到最后两个包后立即回复ack（delay ack凑够两个也立即ack），那么剩下的1436也很快发出去（根据nagle算法，没有没ack的包了，立即发）

100,000 bytes:
前面68个整包很快发出去也收到ack回复了，然后发了第69个整包，剩下88bytes根据nagle算法要等一等，server收到第69个ack后，因为delay ack不回复（手里只攒下一个没有回复的包），所以client、server两边等在等，一直等到server的delay ack超时了。




#### node应用场景及技术
+ https://css-tricks.com/server-side-visualization-with-nightmare/ 这篇介绍了使用nightmare(headless browser,服务端渲染)来做页面自动化测试方案和自动在服务器端根据页面内容生成图片等
+ 
